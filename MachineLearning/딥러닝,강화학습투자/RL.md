# 강화학습

## 강화학습의 기초가 된 마르코프 의사 결정 과정

강화 학습은 Markov Decision Process, MDP에 학습 개념을 넣은 것

### 마르코프 가정

Markov Assumption,

상태가 연속적인 시간에 따라 이어질 때 어떠한 시점의 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정

현재 상태는 바로 이전 상태, 바로 이전 상태는 그 이전 상태에 영향을 받는다고 생각하면 합리적인 가정일 수 있음

마르코프 가정을 통해 어려운 문제를 단순화하고 만족스러운 결과를 얻을 수 있음

P(St|S1,S2,...,St-1) = P(St|St-1)

### 마르코프 과정

Markov Process

마르코프 과정은 마르코프 가정을 만족하는 연속적인 일련의 상태

일련의 상태 <S1, S2, ..., St>와 상태 전이 확률(state transition probability)로 구성

상태 전이 확률 Ps,s' = P[St+1 = s'|St=s] 는 어떤 상태가 i일 때 그 다음 상태가 j가 될 확률을 의미한다.

### 그래서 마르코프 의사 결정 과정이 뭔데 ?

마르코프 과정을 토대로 한 의사 결정 모델

상태(state) 집합 S, 행동(action) 집합 A, 상태 전이 확률(state transition probability) 행렬 P, 보상(reward) 함수 R, 할인 요인 (discount factor) 𝜸로 구성됨

#### 상태 집합

상태 집합은 MDP에서 가질 수 있는 모든 상태의 집합

어떠한 시점에서의 상태 St는 상태 집합 S에 포함된 특정 상태가 됨

#### 행동 집합

행동 주체인 에이전트가 할 수 있는 모든 행동의 집합

에이전트는 어떠한 시점에서의 행동 At를 취함

#### 상태 전이 확률

마르코프 과정에서보다 조금 더 복잡

P^a \_ (s,s') = P [St+1 = s'| St=s,At=a

에이전트가 s에서 a를 취할 때 s'로 변할 확률

#### 보상 함수

에이전트가 어떠한 상태에서 취한 행동에 대한 보상을 내리기 위한 함수

R^a_s = E[Rt+1|St+s, At=a]

상태 s에서 행동 a를 했을 때의 보상의 기댓값을 수치로 변환

#### 할인 요인

과거의 행동을 얼마나 반영할지를 정하는 값으로 0에서 1사이의 값

과거 5번의 행동에 대한 보상을 1씩 받았다고 했을 때 할인 요인이 1이면 <1,1,1,1,1>, 0.9면 <1, 0.9, 0.81, 0.729, 0.6561>

#### 정책

에이전트는 어떤 상태 s에서 수행할 행동 a를 정해야 하는데, 이것을 정책(policy)라고 함

π(a|s) = P[At = a | St = s ]

정책은 총 보상을 최대화하는 방향으로 갱신

#### 상태 가치 함수 (state - value function)

에이전트가 어떤 행동을 수행하면서 상태가 시간에 따라 변함

이때 보상을 받게 되고 시간에 따라 할인된 보상을 더해서, 얻는 가치는 다음과 같이 표현

V_π(s) = E[∑𝜸^i Rt+i+1| St = s]

정책 π에 의해 행동이 결정되고, 결정된 행동에 의해 상태가 정해짐

상태 s에서 π를 따랐을 때의 가치 반환

#### 상태-행동 가치 함수 (action - value function)

어떠한 상태 s에서 행동 a를 수행했을 때의 가치 반환,

Q-value라고도 함

s에서 (정책 π에 따라) a를 수행했을 때 획득할 보상의 총 기댓값

Q_π(s,a) = E[∑𝜸^iRt+i+1 | St=s, At=a]
